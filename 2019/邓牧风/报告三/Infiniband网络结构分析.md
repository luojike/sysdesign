# Infiniband网络结构分析

## 目录

### 1.[Infiniband介绍](#Infiniband介绍)

### 2.[Infiniband网络结构](# Infiniband网络结构)

### 3.[Infiniband构成的集群网络](# Infiniband构成的集群网络)







---

### 1.Infiniband介绍

​	1999年开始起草规格及标准规范，2000年正式发表，但发展速度不及Rapid I/O、PCI-X、PCI-E和FC，加上Ethernet从1Gbps进展至10Gbps。所以直到2005年之后，InfiniBand Architecture(IBA)才在集群式超级计算机上广泛应用。全球HPC高算系统TOP500大效能的超级计算机中有相当多套系统都使用上IBA。

​	 **InfiniBand**是一个用于高性能计算的计算机网络通信标准，它具有极高的吞吐量和极低的延迟，用于计算机与计算机之间的数据互连。InfiniBand也用作服务器与存储系统之间的直接或交换互连，以及存储系统之间的互连。 

​	**基本特征**：InfiniBand技术不是用于一般网络连接的，它的主要设计目的是针对服务器端的连接问题的。因此，InfiniBand技术将会被应用于服务器与服务器（比如复制，分布式工作等），服务器和存储设备（比如SAN和直接存储附件）以及服务器和网络之间（比如LAN， WANs和the Internet）的通信。如下图：

​	![image1](D:\计算机系统设计\课程报告\image3\image1.jpg)

​	 InfiniBand是一个统一的互联结构，既可以处理存储I/O、网络I/O，也能够处理进程间通信。它可以将磁盘阵列、SANs、LANs、服务器和集群服务器进行互联，也可以连接外部网络（比如WAN、VPN、互联网）。设计InfiniBand的目的主要是用于企业数据中心，大型的或小型的。目标主要是实现高的可靠性、可用性、可扩展性和高的性能。InfiniBand可以在相对短的距离内提供高带宽、低延迟的传输，而且在单个或多个互联网络中支持冗余的I/O通道，因此能保持数据中心在局部故障时仍能运转。 

​	 InfiniBand的基本带宽是2.5Gb/s，这是InfiniBand 1.x。InfiniBand是全双工的，因此在两个方向上的理论最大带宽都是2.5Gb/s，总计5Gb/s。与此相反，PCI是半双工，因此32位、33MHz的PCI总线单个方向上能达到的理论最大带宽是1Gb/s，64位、133MHz的PCI-X总线能达到8.5Gb/s，仍然是半双工。当然，任何一种总线的实际吞吐量从来没有达到理论最大值。

​	 InfiniBand是在串行链路上实现超高速率的，因此电缆和连接器相对并行I/O接口PCI、IDE/ATA、SCSI和IEEE-1284来说，接口小也便宜。并行链路有一个固有的优势，因为它的多个缆线相当于高速公路上的多个车道，但现代的I/O收发器芯片使串行链路达到更高的数据速率，并且价格便宜。这就是为什么现在的通信技术──InfiniBand、IEEE-1394、串行ATA、串行连接SCSI、USB大多是采用串行I/O而不是并行I/O。

​	 InfiniBand的扩展性非常高，在一个子网内可支持上万个节点，而每个网络中可有几千个子网，每个安装的系统中可以有多个网络结构。InfiniBand交换机通过子网路由分组，InfiniBand路由器将多个子网连接在一起。相对以太网，InfiniBand可以更加分散地进行管理，每个子网内有一个管理器，其在路由分组、映射网络拓扑、在网络内提供多个链路、监视性能方面起决定性的作用。子网管理器也能保证在特别通道内的带宽，并为不同优先权的数据流提供不同级别的服务。子网并不一定是一个单独的设备，它可以是内置于交换机的智能部件。 

**Infiniband的优势**：

* Infiniband大量用于FC/IP SAN、NAS和服务器之间的连接,作为iSCSI RDMA的存储协议iSER已被IETF标准化。目前EMC全系产品已经切换到Infiniband组网，IBM/TMS的FlashSystem系列，IBM的存储系统XIV Gen3，DDN的SFA系列都采用Infiniband网络。它的性能是FC的3.5倍，Infiniband交换机的延迟是FC交换机的1/10，同时也支持SAN和NAS。
* 现在的存储系统已不能满足于 传统的FC SAN所提供的服务器与裸存储的网络连接架构。HP SFS和IBM GPFS 是在Infiniband fabric连接起来的服务器和iSER Infiniband存储构建的并行文件系统，完全突破系统的性能瓶颈。 
* 同时Infiniband采用的是PCI串行高速宽带链接，从SDR、DDR、QDR、FDR、到EDR HCA连接，都可以做到1微秒，甚至纳秒级别的极低时延，基于链路层的流控机制也实现先进的拥塞控制。



### 2.Infiniband网络结构

 	Infiniband的协议采用分层结构，是一种分层协议，类似于TCP/IP协议， 每层负责不同的功能，下层为上层服务，不同层次相互独立 。如下图所示：

​	![image2](D:\计算机系统设计\课程报告\image3\image2.jpg)

​	其中,**物理层**定义了在线路上如何将比特信号组 成符号,然后再组成帧、数据符号以及包之间的数据填 充等,详细说明了构建有效包的信令协议等；

​	**链路层**定义了数据包的格式以及数据包操作的协议,如流控、路由选择、编码、解码等；

​	**网络层**通过在数据包上添加一个40字节的全局的路由报头(Global Route Header,GRH)来进行路由的选择，对数据进行转发。在转发的过程中，路由器仅仅进行可变的CRC校验,这样就保证了端到端的数据传输的完整性；

​	**传输层**再将数据包传送到某个指定 的队列偶(QueuePair,QP)中,并指示QP如何处理该数据 包以及当信息的数据净核部分大于通道的最大传输单 元MTU时,对数据进行分段和重组。

**Infiniband的基本组件：**

* HCA（Host Channel Adapter），它是连接内存控制器和TCA的桥梁； 
* TCA(Target Channel Adapter)，它将I/O设备（例如网卡、SCSI控制器）的数字信号打包发送给HCA； 
* Infiniband link，它是连接HCA和TCA的光纤，InfiniBand架构允许硬件厂家以1条、4条、12条光纤3种方式连结TCA和HCA； 
* 交换机和路由器； 

 其网络拓扑结构如下图：

 ![image3](D:\计算机系统设计\课程报告\image3\image3.jpg)



​		 

### 3.Infiniband构成的集群网络

​	**胖树拓扑结构**是搭建大规模Infiniband网络的常用网络拓扑结构。胖树拓扑结构一方面能够提供非阻塞传输，具有可扩展性强和易管理等优点，而另一方面它往往会造成通信路线的冗余。

​	对于大型集群系统，必须要考虑交换机之间的网络互连结构。通常由于树型拓扑具有结构清晰、易于搭建和便于管理等优点，目前已经被广泛的应用到各个领域。胖树是树型拓扑结构的一种，它可以使网络中每一层都具有相等的带宽，从而提供非阻塞网络传输，可以有效改善通信的拥塞状况。

​	可以把胖树定义为k元n树的规则拓扑。胖树用于交换机内部开关的连接时，k元n树的深度为n，树中的每一个内部开关都有k个上行链路和k个下行链路，分别用于连接它们的祖先和子孙。这时的顶点不再是唯一的，它更像是由多棵树交错而成，如果将顶层所以的开关看作是一个开关集的话，那么它就是一个棵拥有多棵子树的胖树。这种连接方式要比二叉胖树具有更强大的连接功能。在具有同等数量开关的前提下，k元n树比二叉胖树连接能够提供更多的连接端口和使用更少的层，可以减少数据传输时需要经过的开关数目，从而降低交换机内部的传输延迟。

​	使用胖树拓扑的集群网络结构一般由叶交换机和主干交换机（包括将各个叶交换机互连起来的交换机）组成。叶交换机与终端节点相连，分配一部分端口给节点，另一部分端口被接入集群系统网络结构内部。如下图：

​	![image4](D:\计算机系统设计\课程报告\image3\image4.jpg)



​	在树型拓扑中，当叶交换机同时向网络发送信息时，需要由主干交换机提供路径，为了减少拥塞，需要利用胖树拓扑的多路径特点，增加各个叶交换机的互连路径，也就是增加主干交换机的数量。在构建大型集群时，由于交换机的数量增长速度较快，随着叶交换机数量的增加，主干交换机的数量也随之增加，但是发生拥塞的概率却将趋近于0。

​	![image5](D:\计算机系统设计\课程报告\image3\image5.jpg)