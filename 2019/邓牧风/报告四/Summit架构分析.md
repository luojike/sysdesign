# Summit架构分析

### 目录

---

### 1.[Summit背景](#summit背景)

### 2.[Summit介绍](#summit介绍)

### 3.[Summit架构](#summit架构)











---

### 1.Summit背景

 为什么Summit超级计算机问世了？这其实也有中国的功劳。

![image4](D:\计算机系统设计\课程报告\image4\image4.jpg)

 超算TOP500每年发布两次，我们国家的天河2号和太湖之光，分别六次和四次拿到冠军，一共十次，相当于连续五年占据了TOP500的冠军位置。所以美国政府也是很着急。之前由于奥巴马政府对超算不够重视，奥巴马只是在第二个任期快结束的时候才发布了一项总统令，加快超算研制的创新步伐。 而特朗普就任以后，却对超算极其重视，在砍掉了很多科学研究预算的情况下，超算的预算不但没砍，反而增加了。

 几年前，美国部署了三台百P（相当于十亿亿次）量级的超级计算机，每秒的运算速度可以达到100P到200P左右，分别是Summit、Sierra、Aurora三台机器。目标有三个：

* 第一当然是为美国的国家实验室研发世界领先的超级计算模拟系统，保持美国科学创新的领导地位；
* 第二是希望保持美国在超级计算机研制上的技术优势和领先水平；
* 第三就是希望把TOP500第一的位置夺回来。 

 所以美国大力推动超级计算的发展，计划在2018年年底才推出的Summit也是提前了半年就推出了。以下就是2018年6月份的超算Top500榜单：

 ![image5](D:\计算机系统设计\课程报告\image4\image5.jpg)





### 2.Summit介绍

 2018年6月8日公布的美国能源部Summit超级计算机迅速燃爆了高性能计算社区，它的双精度浮点理论峰值性能超过200 PFLOPS （Top500官网文章说是215 PFLOPS ），大大超过了太湖之光的125 PFLOPS ， 对于某些科学应用，Summit还可以每秒进行超过30亿次混合精度计算。Summit的主要架构是CPU+GPU，其中CPU是IBM Power 9，GPU是NVIDIA Tesla V100。全系统共4608节点，其中每个节点包含2 CPU+6 GPU，并与双轨Mellanox EDR 100Gb/s InfiniBand互连。Summit还拥有10 PB以上的内存，并配有快速，高带宽的通道，可以高效地进行数据移动 。根据每块[Tesla V100]( https://github.com/luojike/sysdesign/blob/master/2019/邓牧风/报告二/TeslaGPU架构分析.md "Tesla V100")能够给出7.8TFLOPS 的双精度浮点性能来计算，那么27648块GPU提供的双精度浮点理论峰值性能就已经达到了215.65 PFLOPS 。因此，Summit本质上就是一个超大规模的V100集群。 

![image1](D:\计算机系统设计\课程报告\image4\image1.jpg)

 橡树岭国家实验室的团队声称Summit是第一台从头开始设计的超级计算机，可以运行AI应用程序，例如机器学习和神经网络。它拥有来自Nvidia的超过27,000个GPU芯片，其增加了大量AI应用程序，并且还包括一些IBM去年推出的Power9芯片，专门用于AI工作负载。另外还有一个超高速通信链路，用于在这些硅芯片之间传输数据。

 ![image2](D:\计算机系统设计\课程报告\image4\image2.jpg)





### 3.Summit架构

 从上面介绍也可以得知每个计算节点都有两个IBM POWER9（P9）CPU和六个NVIDIA Volta V100 GPU，并与Mellanox EDR IB网络相连。采用的是FATTURE网络拓扑 。

![image6](D:\计算机系统设计\课程报告\image4\image6.jpg)

* #### 节点设计

  Summit的计算节点有两个IBM 3.07GHz POWER9 CPUs，每个CPU有22个核，强大的CPU内核减少了固有的顺序代码区域的影响，并使应用程序从同构体系结构平稳过渡。套接字由IBM的X-Bus连接，它在套接字之间提供64GB/s的一致访问。每个套接字有八个内存通道，分别连接Summit上的256 GB DDR4和Sierra上的128 GB DDR4，这两个通道都为每个节点提供340 GB/s的峰值内存带宽。每个节点还包括一个1.6TB Samsung NVMe固态硬盘，用作写缓存（即突发缓冲区），也可以通过mmap用于本地暂存存储、缓存库或扩展内存。

  每个节点具有六个NVIDIA Volta V100 GPUs，每个GPU都具有80个1.333GHz流式多处理器（SM），其总的峰值性能为7 TF 双精度和14 TF 单精度。下表就是Summit的特征：

  |        **节点数**        |      **4608**       |
  | :----------------------: | :-----------------: |
  |       **峰值性能**       |     **200 PF**      |
  |     **GPU的总内存**      |   **442 TB HBM2**   |
  |     **DDR的总内存**      |     **2.4 PB**      |
  | **互连Bi-Section的带宽** | **EDR IB 115 TB/s** |
  |       **拓扑结构**       |   **1：1的胖树**    |
  |    **突发缓冲区容量**    |     **7.4 PB**      |
  |    **突发缓冲区带宽**    |    **9.7 TB/s**     |
  |     **文件系统容量**     |     **250 PB**      |
  |     **文件系统带宽**     |    **2.5 TB/s**     |

  GPU还包括112个TF张量核，它们在单精度累加的半精度输入上执行4X4矩阵乘法。每个POWER9都有150 GB/s的NVLink连接，由三个GPU共享。还提供了连接到相同CPU插槽的GPU之间的NVLink，速率为50GB/s。在这两种情况下，NVLink都能实现更快的内部数据移动，与Titan上有限的PCIe带宽相比，这将有利于支持GPU的应用程序。 

  除了使用NVLink 2改进延迟和带宽之外，节点还提供一个包括系统存储器和GPU HBM2存储器的单相干地址空间。应用程序（或OpenMP运行时）可以通过使用cudaMalloc()和cudaMemcpy()在系统内存和GPU内存之间移动数据来显式地管理内存，也可以使用cudaMallocManaged()让CUDATM运行时管理副本。单地址空间允许应用程序使用malloc()，然后将指针传递给GPU内核，这大大简化了应用程序的移植。下面总结一下Summit节点的主要特征：

  |        **CPU**         |   **2个POWER9**   |
  | :--------------------: | :---------------: |
  |         **核**         |     **44个**      |
  |        **内存**        |    **512 GB**     |
  |      **内存带宽**      |   **340 GB/s**    |
  |      **SMP 总线**      | **X-Bus 64 GB/s** |
  |        **GPU**         | **6个Volta V100** |
  | **流式多处理器（SM）** |      **480**      |
  |      **GPU内存**       |  **96 GB HBM2**   |
  |    **GPU内存带宽**     |   **5.4 TB/s**    |
  |     **NVLink带宽**     |  **50 GB/s/GPU**  |
  |    **固态硬盘容量**    |    **1.6 TB**     |
  |   **固态硬盘写带宽**   |   **2.1 GB/s**    |

  

* #### 互连设计

  Mellanox EDR胖树网络拓扑使用SwitchIBTM 2交换机和ConnectXTM-5主机通道适配器（HCA）。计算机架有18个计算节点，每个节点都有一个双端口Mellanox ConnectX-5 HCA。每个节点的HCA端口连接到单独的TOR交换机（每个计算机架两个），并在每个计算机架内提供无阻塞连接。所有tor连接到一组核心交换机，从而创建单个InfiniBand子网。Summit的结构在TOR和核心交换机之间是完全无阻塞的。

  随着更多的技术投入，Summit显著提高了网络性能。新的或改进的功能包括：自适应路由、基于交换机的集体卸载（SHARPTM）、动态连接传输（DCT）、HCAs中的标记匹配和GPU DirectTM。这些特征组合以提供最大的业务吞吐量、更低的延迟通信、增强的集体性能、以及改进的GPU到GPU通信。

  

* #### I/O设计

  Summit系统架构包括一个两层I/O子系统，它由节点上的突发缓冲区（BBs）和一个并行文件系统（PFS）组成，以满足系统I/O需求。这些层表示为两个独立的名称空间。OLCF和LLNL与IBM合作，通过几个NRE活动改进I/O子系统的设计。这些活动包括BB和容量层（BB-API）实现数据移动机制，以及在节点本地ssd上分层以支持N-1 I/O访问模式的专用分布式文件系统（bscf）。其他NRE活动提高了PFS文件系统的扩展性和单目录共享元数据的性能。

  虽然其他系统，包括NERSC的Cori，早已经部署了BBs，但是IBM系统架构将是第一个大规模使用节点本地设备的系统。与共享BB层相比，node-local选项需要更少的基础设施（如服务器、网络），并且可以减少数据移动（仅每N个检查点排出一次），并提高N-N用例的性能。虽然共享BBs简化了N-1用例，但是节点本地软件可以利用分阶段并行I/O访问模式来提供类似的功能。

   如前所述，每个计算节点都包括一个1.6 TB的NVMe驱动器，它提供高达2.1GB/s的写入和5.8GB/s的读取I/O性能。因此，Summit的总容量为7.4 PB（总系统内存的2.5倍），写入性能为9.7 TB/s。

  应用程序可以在不同的抽象级别和易用性上与BB层交互：在节点本地SSD上构建的XFS文件系统，在作业分配时创建；直接通过BB-API或BSCFS（用于共享文件）；或通过SCR或其他构建在BB-API上的库。硬件辅助的NVMe over Fabrics（NVMeoF）使Mellanox HCA能够直接在BB上移动数据，而无需计算节点CPU开销。QoS和限制减轻了网络影响。应用程序可以在作业分派之前将数据预处理到BB。当数据被异步地排放到PFS时，它们可以检查并立即恢复计算。

  检查点通常写为多个文件（即N-N）或一个共享文件（即N-1）。虽然每个进程的文件I/O直接映射到节点本地BB，但是共享文件用例更具挑战性。BSCFS是支持N-1检查点的每个作业的分布式日志结构文件系统。BSCFS在SSD中缓存共享文件，然后在PFS中重新构建它们，而无需应用程序的进一步干预。

  容量层使用IBM的SpectrumScale GPFSTM产品，它具有NRE增强功能，例如更大的文件系统块大小、更多的文件系统子块、减少GPFS锁争用以及提高文件系统工具（如parallel fsck）的可扩展性。Spectrum Scale的单目录共享元数据创建性能提高了10倍，达到每秒50000个元数据操作。此外，使用其“分散”模式（使块分配随机化）可提高随机I/O性能（更好地反映应用程序I/O模式），并j减少随着PFS的老化而导致的性能下降。