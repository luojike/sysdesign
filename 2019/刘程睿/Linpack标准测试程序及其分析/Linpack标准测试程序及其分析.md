# Linpack标准测试程序及其分析
## Linpack简介
  Linpack是国际上使用最广泛的测试高性能计算机系统浮点性能的基准测试。通过对高性能计算机采用高斯消元法求解一元 N次稠密线性代数方程组的测试，评价高性能计算机的浮点计算性能。Linpack的结果按每秒浮点运算次数（flops）表示。
  Linpack 测试包括三类，Linpack100、Linpack1000和HPL。Linpack100求解规模为100阶的稠密线性代数方程组，它只允许采用编译 优化选项进行优化，不得更改代码，甚至代码中的注释也不得修改。Linpack1000要求求解1000阶的线性代数方程组，达到指定的精度要求，可以在 不改变计算量的前提下做算法和代码上做优化。HPL即High Performance Linpack，也叫高度并行计算基准测试，它对数组大小N没有限制，求解问题的规模可以改变，除基本算法（计算量）不可改变外，可以采用其它任何优化方法。前两种测试运行规模较小，已不是很适合现代计算机的发展。
## HPL简介及操作
### 概述
HPL，即High Performance Linpack，目前已经成为国际标准的Linpack基准测试程序，其1.0版于2000年9月发布，是第一个标准的公开版本并行Linpack测试软件包，一般用于全世界TOP500超级计算机上的并行超级计算机排名。HPL测试标准的用户自由度要大很多，使用者可以选择矩阵的规模，分块大小，分解方法等等一系列的各种参数，都是按需要更改的。 
HPL软件包需要在配备了MPI环境下的系统中才能运行，还需要底层有线性代数子程序包BLAS的支持（或者有另一种向量信号图像处理库VSIPL也可）。
HPL软件包不仅提供了完整的Linpack测试程序，还进行了全面细致的计时工作，最后可以得到求解的精确性和计算所花费的总时间。该软件在系统上所能达到的最佳性能值适合很多因素有关的。
### 主算法
该软件包是用来求一个N维的线性方程组A x = b的解，首先通过选局部列主元的方法对Nⅹ(N+1)的[A b]系数矩阵进行LU分解成如下形式：[A b]=[[L,U] y]
由于下三角矩阵L因子所作的变换在分解的过程中也逐步应用到b上，所以最后方程组的解x就可以由转化为求解上三角矩阵U作为系数矩阵的线性方程组 U x = y从而得到。
为了保证良好的负载平衡和算法的可扩展性，数据是以循环块的方式分布到一个P x Q的由所有进程组成的2维网格中。N x (N+1)的系数矩阵首先在逻辑上被分成一个个Nb x Nb大小的数据块，然后循环的分配到P x Q进程网格上去处理。这个分配的工作在矩阵的行、列两个方向同时进行。如下图所示：

![主算法](https://github.com/luojike/sysdesign/blob/master/2019/%E5%88%98%E7%A8%8B%E7%9D%BF/Linpack%E6%A0%87%E5%87%86%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E5%8F%8A%E5%85%B6%E5%88%86%E6%9E%90/image/%E4%B8%BB%E7%AE%97%E6%B3%95.png)

（左图为整个矩阵被分成小块以后，右图为各小块分配到各个进程后的情况）

在LU分解的主循环中使用的是向右看（right-looking）的分解法，也就是说，在每次迭代过程对一个nb列的板块分解，然后剩余的子矩阵被更新。因此，我们也可以注意到这里的板块在逻辑上被分为nb大小的子块，而这个Nb恰好就是前面数据分布中使用的Nb。
### 安装
搭建并行环境并且安装hpl

![安装1](https://github.com/luojike/sysdesign/blob/master/2019/%E5%88%98%E7%A8%8B%E7%9D%BF/Linpack%E6%A0%87%E5%87%86%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E5%8F%8A%E5%85%B6%E5%88%86%E6%9E%90/image/%E5%AE%89%E8%A3%851.png)
![安装2](https://github.com/luojike/sysdesign/blob/master/2019/%E5%88%98%E7%A8%8B%E7%9D%BF/Linpack%E6%A0%87%E5%87%86%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E5%8F%8A%E5%85%B6%E5%88%86%E6%9E%90/image/%E5%AE%89%E8%A3%852.png) 

修改Make.Linux文件，使其能够编译通过

![安装3](https://github.com/luojike/sysdesign/blob/master/2019/%E5%88%98%E7%A8%8B%E7%9D%BF/Linpack%E6%A0%87%E5%87%86%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E5%8F%8A%E5%85%B6%E5%88%86%E6%9E%90/image/%E5%AE%89%E8%A3%853.png)
### 执行

修改HPL.dat文件

![执行](https://github.com/luojike/sysdesign/blob/master/2019/%E5%88%98%E7%A8%8B%E7%9D%BF/Linpack%E6%A0%87%E5%87%86%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E5%8F%8A%E5%85%B6%E5%88%86%E6%9E%90/image/%E6%89%A7%E8%A1%8C.png)
### 执行结果
![结果1](https://github.com/luojike/sysdesign/blob/master/2019/%E5%88%98%E7%A8%8B%E7%9D%BF/Linpack%E6%A0%87%E5%87%86%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E5%8F%8A%E5%85%B6%E5%88%86%E6%9E%90/image/%E7%BB%93%E6%9E%9C.png) 

终端上的最终输出将如上所示。 最后一个值给出了速度和值之前显示提供的不同参数。 在上面的内容中，速度显示在Gflops中，其值约为7.1377e+00 Gflops（GFLOPS 就是 Giga Floating-point Operations Per Second,即每秒10亿次的浮点运算数），当转换时提供7137兆FLOPS（MFLOPS）。
当将NS改为1040时，显示的Gflops为8.7712e+00，当Ns降低时，计算速度提升

![结果2](https://github.com/luojike/sysdesign/blob/master/2019/%E5%88%98%E7%A8%8B%E7%9D%BF/Linpack%E6%A0%87%E5%87%86%E6%B5%8B%E8%AF%95%E7%A8%8B%E5%BA%8F%E5%8F%8A%E5%85%B6%E5%88%86%E6%9E%90/image/%E7%BB%93%E6%9E%9C2.png) 

查阅资料发现，当矩阵的规模N越大，有效计算所占的比例也越大，系统浮点处理性能也就越高；但与此同时，矩阵规模的增加会导致内存消耗量的增加，一旦系统实际内 存空间不足，使用缓存、性能会大幅度降低。因此，对于一般系统而言，要尽量增大矩阵规模N的同时，又要保证不使用系统缓存。因为操作系统本身需要占 用一定的内存，除了矩阵（N×N）之外，HPL还有其他的内存开销，另外通信也需要占用一些缓存。矩阵占用系统总内存的80%左右为最佳，即N×N×8=系统总内存×80%。
NB：为提高数据的局部性，从而提高整体性能，HPL采用分块矩阵的算法。分块的大小对性能有很大的影响，NB的选择和软硬件许多因素密切相关。NB值的选择主要是通过实际测试得到最优值。
